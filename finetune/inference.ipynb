{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0ceba8",
   "metadata": {},
   "source": [
    "#### Installing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6df04-cbed-48ea-b378-4b8a8a3b5677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install sentencepiece\n",
    "!pip install bitsandbytes==0.43.1\n",
    "!pip install accelerate==0.29.3\n",
    "!pip install peft --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64244282",
   "metadata": {},
   "source": [
    "#### Logging into huggingface for model access and also setting env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0384ca16-3066-4212-83d9-51e276e08083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "auth_token = \"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\"\n",
    "login(token=auth_token)\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = auth_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c6b56",
   "metadata": {},
   "source": [
    "#### Loading the base model and adapter, and merging them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dcee5f2-9e7d-44b7-976e-b3fa22f8ece5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e4af04268c4da08e87f11071a8616e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f647e217a4493383dd9939baedd56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:  42%|####1     | 2.10G/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b1a1f0f82747fcb3d45afbc86f9418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabfcc901fce4c75ac0592bf899b8241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb2568076a744b9b8f099fff28ee1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d25615df9424a0a8f480c31b496b57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687b4007013845278f131bf7769e26c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, MistralForCausalLM, LlamaTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "peft_model = \"Burhan02/Mistral-7B-Instruct-v0.2-ft\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(base_model, peft_model)\n",
    "\n",
    "model.merge_and_unload()\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19dd33",
   "metadata": {},
   "source": [
    "#### Initializing the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be652f-c447-4e25-80b6-7a3d1dfa1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31095b48",
   "metadata": {},
   "source": [
    "#### defining the inference function for the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(user_prompt, i):\n",
    "    sys_prompt = \"\"\"\n",
    "    You are an agent responsible for building and runnning nerual networks solely in the PyTorch library.\n",
    "    The user gives you 3 inputs:\n",
    "    1. Layers: this is the layer by layer breakdown of the neural network. This includes things like the dimensionality \n",
    "    of the (input size, output size).\n",
    "    2. Task: This is either regression, binary classification or multiclass-classification \n",
    "    (with the number of classes specified by the user).\n",
    "    3. Data Path: local path to the data.csv file (always a csv).\n",
    "\n",
    "    Given these inputs, your job is to generate a .py file that contains the following:\n",
    "    1. A neural_net class in pytorch which includes these methods:\n",
    "        a. init\n",
    "        b. forward \n",
    "        c. train\n",
    "        d. predict\n",
    "    2. Code to read in the dataframe from data_path given by the user.\n",
    "    3. Code to split the dataframe into train and test set according to the user task.\n",
    "    4. Code to train the the neural net you created to train on train set and predict on test set.\n",
    "\n",
    "    Make sure to enclose your output in ''' at the beginning and end. ONLY GENERATE THE .py CODE FILE CONTAINING THE PYTORCH CODE.\n",
    "    DO NOT EXPLAIN YOURSELF.\n",
    "    \"\"\"\n",
    "\n",
    "    updated_prompt = sys_prompt + user_prompt\n",
    "\n",
    "    # tokenizing the prompt and sending to gpu\n",
    "    inputs = tokenizer(updated_prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    # performing generation on the finetuned model using prompt as input\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(**inputs, max_length=6000)\n",
    "\n",
    "    # decoding the response from the model\n",
    "    outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    # saving the generated code in a .py file\n",
    "    with open(f'output_{i}.py', 'w') as file:\n",
    "        code = outputs.split(\"'''\")[2]\n",
    "        file.write(code)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e00c4",
   "metadata": {},
   "source": [
    "#### calling inference on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt1 = \"\"\"\n",
    "Layers:\n",
    "    - Fully connected layer 1:\n",
    "    - Input size: 10\n",
    "    - Output size: 128\n",
    "    - Fully connected layer 2:\n",
    "    - Input size: 128\n",
    "    - Output size: 256\n",
    "    - Fully connected layer 3:\n",
    "    - Input size: 256\n",
    "    - Output size: 128\n",
    "    - Fully connected layer 4 (readout layer):\n",
    "    - Input size: 128\n",
    "    - Output size: 4\n",
    "\n",
    "    Activation function between hidden layers except last:\n",
    "        - ReLU\n",
    "\n",
    "    Apply softmax to of size 4 after output layer\n",
    "        \n",
    "    Task:\n",
    "        - Multiclass Classification with 4 classes.\n",
    "\n",
    "    Data Path: \"./root/data.csv\"\n",
    "\"\"\"\n",
    "\n",
    "user_prompt2 = \"\"\"\n",
    "Layers:\n",
    "Transformer Encoder layers: \n",
    "    Input feature_size --> output feature_size\n",
    "Linear function: \n",
    "    Input feature_size (output of Transformer) --> 1 (for regression)\\\n",
    "\n",
    "Task: Multivariate regression\n",
    "\n",
    "Data Path: './root/data.csv'\n",
    "\n",
    "Target Column: target_col\n",
    "\"\"\"\n",
    "\n",
    "user_prompt3 = \"\"\"\n",
    "Layers:\n",
    "Convolution layer:\n",
    "    Input size: (num_nodes, batch_size, input_seq_len, in_feat)\n",
    "    Output size: (num_nodes, batch_size, input_seq_len, out_feat)\n",
    "\n",
    "LSTM layer:\n",
    "    Input size: (num_nodes, input_seq_len, out_feat)\n",
    "    Output size: (num_nodes, output_seq_len, lstm_units)\n",
    "\n",
    "Dense layer (readout layer):\n",
    "    Input size: (num_nodes, output_seq_len, lstm_units)\n",
    "    Output size: (batch_size, output_seq_len, num_nodes)\n",
    "\n",
    "Task:\n",
    "Regression. Model for forecasting over the graph. Predicting the future values of the traffic speed given a history of the traffic speed for a collection of road segments.\n",
    "    \n",
    "Data Path: '/root/data.csv'\n",
    "\"\"\"\n",
    "\n",
    "infer(user_prompt1, 1)\n",
    "infer(user_prompt2, 2)\n",
    "infer(user_prompt3, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
