{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b3b571-60bf-4600-8f85-f1556d30d1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.1.2\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (3.1.2)\n",
      "Collecting fsspec (from torch==2.1.2)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch==2.1.2)\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.0.0\n",
      "    Uninstalling triton-2.0.0:\n",
      "      Successfully uninstalled triton-2.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1+cu118\n",
      "    Uninstalling torch-2.0.1+cu118:\n",
      "      Successfully uninstalled torch-2.0.1+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.1.2 which is incompatible.\n",
      "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.3.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 triton-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Obtaining axolotl from git+https://github.com/OpenAccess-AI-Collective/axolotl#egg=axolotl\n",
      "  Cloning https://github.com/OpenAccess-AI-Collective/axolotl to ./src/axolotl\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/OpenAccess-AI-Collective/axolotl /workspace/src/axolotl\n",
      "  Resolved https://github.com/OpenAccess-AI-Collective/axolotl to commit 601c08b4c2d9a8198527e5e33536d3ad499305f0\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers@ git+https://github.com/huggingface/transformers.git@43d17c18360ac9c3d3491389328e2fe55fe8f9ce (from axolotl)\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision 43d17c18360ac9c3d3491389328e2fe55fe8f9ce) to /tmp/pip-install-4fmq0okj/transformers_a2158e7a8ae34e8996f476b27f7da75e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-4fmq0okj/transformers_a2158e7a8ae34e8996f476b27f7da75e\n",
      "  Running command git rev-parse -q --verify 'sha^43d17c18360ac9c3d3491389328e2fe55fe8f9ce'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers.git 43d17c18360ac9c3d3491389328e2fe55fe8f9ce\n",
      "  Running command git checkout -q 43d17c18360ac9c3d3491389328e2fe55fe8f9ce\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 43d17c18360ac9c3d3491389328e2fe55fe8f9ce\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8 (from axolotl)\n",
      "  Cloning https://github.com/lm-sys/FastChat.git (to revision 5095615810cf613dba7f27dd155f571fcff976d8) to /tmp/pip-install-4fmq0okj/fschat_55d40c99d2ba429abcd4dc2277506942\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/lm-sys/FastChat.git /tmp/pip-install-4fmq0okj/fschat_55d40c99d2ba429abcd4dc2277506942\n",
      "  Running command git rev-parse -q --verify 'sha^5095615810cf613dba7f27dd155f571fcff976d8'\n",
      "  Running command git fetch -q https://github.com/lm-sys/FastChat.git 5095615810cf613dba7f27dd155f571fcff976d8\n",
      "  Running command git checkout -q 5095615810cf613dba7f27dd155f571fcff976d8\n",
      "  Resolved https://github.com/lm-sys/FastChat.git to commit 5095615810cf613dba7f27dd155f571fcff976d8\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging==23.2 in /usr/local/lib/python3.10/dist-packages (from axolotl) (23.2)\n",
      "Collecting peft==0.10.0 (from axolotl)\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tokenizers==0.15.0 (from axolotl)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting bitsandbytes==0.43.0 (from axolotl)\n",
      "  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting accelerate==0.28.0 (from axolotl)\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pydantic==2.6.3 (from axolotl)\n",
      "  Downloading pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting addict (from axolotl)\n",
      "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fire (from axolotl)\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.10/dist-packages (from axolotl) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from axolotl) (2.31.0)\n",
      "Collecting datasets==2.15.0 (from axolotl)\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece (from axolotl)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting wandb (from axolotl)\n",
      "  Downloading wandb-0.16.6-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting einops (from axolotl)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting optimum==1.16.2 (from axolotl)\n",
      "  Downloading optimum-1.16.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting hf_transfer (from axolotl)\n",
      "  Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting colorama (from axolotl)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting numba (from axolotl)\n",
      "  Downloading numba-0.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting numpy>=1.24.4 (from axolotl)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate==0.4.1 (from axolotl)\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting scipy (from axolotl)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn==1.2.2 (from axolotl)\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pynvml (from axolotl)\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting art (from axolotl)\n",
      "  Downloading art-6.2-py3-none-any.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gradio==3.50.2 (from axolotl)\n",
      "  Downloading gradio-3.50.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting tensorboard (from axolotl)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting s3fs (from axolotl)\n",
      "  Downloading s3fs-2024.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting gcsfs (from axolotl)\n",
      "  Downloading gcsfs-2024.3.1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting trl==0.8.5 (from axolotl)\n",
      "  Downloading trl-0.8.5-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting zstandard==0.22.0 (from axolotl)\n",
      "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting fastcore (from axolotl)\n",
      "  Downloading fastcore-1.5.33-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.10/dist-packages (from axolotl) (2.1.2)\n",
      "Collecting xformers>=0.0.23 (from axolotl)\n",
      "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0->axolotl) (5.9.6)\n",
      "Collecting huggingface-hub (from accelerate==0.28.0->axolotl)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate==0.28.0->axolotl)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow>=8.0.0 (from datasets==2.15.0->axolotl)\n",
      "  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.15.0->axolotl)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0->axolotl)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets==2.15.0->axolotl)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting tqdm>=4.62.1 (from datasets==2.15.0->axolotl)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets==2.15.0->axolotl)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.15.0->axolotl)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0->axolotl)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.15.0->axolotl)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting responses<0.19 (from evaluate==0.4.1->axolotl)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio==3.50.2->axolotl)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio==3.50.2->axolotl)\n",
      "  Downloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting fastapi (from gradio==3.50.2->axolotl)\n",
      "  Downloading fastapi-0.110.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ffmpy (from gradio==3.50.2->axolotl)\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==0.6.1 (from gradio==3.50.2->axolotl)\n",
      "  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx (from gradio==3.50.2->axolotl)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio==3.50.2->axolotl)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (2.1.2)\n",
      "Collecting matplotlib~=3.0 (from gradio==3.50.2->axolotl)\n",
      "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting orjson~=3.0 (from gradio==3.50.2->axolotl)\n",
      "  Downloading orjson-3.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (9.3.0)\n",
      "Collecting pydub (from gradio==3.50.2->axolotl)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart (from gradio==3.50.2->axolotl)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio==3.50.2->axolotl)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (4.4.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio==3.50.2->axolotl)\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio==3.50.2->axolotl)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting coloredlogs (from optimum==1.16.2->axolotl)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum==1.16.2->axolotl) (1.12)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic==2.6.3->axolotl)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic==2.6.3->axolotl)\n",
      "  Downloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions~=4.0 (from gradio==3.50.2->axolotl)\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn==1.2.2->axolotl)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.2.2->axolotl)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (3.9.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->axolotl) (2.1.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.8.5->axolotl)\n",
      "  Downloading tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->axolotl) (12.4.127)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl) (2022.12.7)\n",
      "Collecting regex!=2019.12.17 (from transformers@ git+https://github.com/huggingface/transformers.git@43d17c18360ac9c3d3491389328e2fe55fe8f9ce->axolotl)\n",
      "  Downloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting xformers>=0.0.23 (from axolotl)\n",
      "  Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Downloading xformers-0.0.25-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire->axolotl) (1.16.0)\n",
      "Collecting termcolor (from fire->axolotl)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting markdown2[all] (from fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl)\n",
      "  Downloading markdown2-2.4.13-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting nh3 (from fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl)\n",
      "  Downloading nh3-0.2.17-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl) (3.0.39)\n",
      "Collecting rich>=10.0.0 (from fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shortuuid (from fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl)\n",
      "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tiktoken (from fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->axolotl) (5.1.1)\n",
      "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gcsfs (from axolotl)\n",
      "  Downloading gcsfs-2024.2.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading gcsfs-2023.12.2.post1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading gcsfs-2023.12.1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading gcsfs-2023.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading gcsfs-2023.10.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting google-auth>=1.2 (from gcsfs->axolotl)\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib (from gcsfs->axolotl)\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-storage (from gcsfs->axolotl)\n",
      "  Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba->axolotl)\n",
      "  Downloading llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->axolotl)\n",
      "  Downloading aiobotocore-2.12.3-py3-none-any.whl.metadata (21 kB)\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs (from axolotl)\n",
      "  Downloading s3fs-2024.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.7.0 (from s3fs->axolotl)\n",
      "  Downloading aiobotocore-2.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->axolotl)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->axolotl)\n",
      "  Downloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->axolotl)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard->axolotl)\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl) (68.2.2)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->axolotl)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard->axolotl)\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting Click!=8.0.0,>=7.1 (from wandb->axolotl)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->axolotl)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb->axolotl)\n",
      "  Downloading sentry_sdk-2.0.1-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->axolotl)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting setproctitle (from wandb->axolotl)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Collecting appdirs>=1.4.3 (from wandb->axolotl)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard->axolotl)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting botocore<1.31.65,>=1.31.16 (from aiobotocore~=2.7.0->s3fs->axolotl)\n",
      "  Downloading botocore-1.31.64-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore~=2.7.0->s3fs->axolotl)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore~=2.7.0->s3fs->axolotl)\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.15.0->axolotl)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0->axolotl) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.15.0->axolotl)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.15.0->axolotl)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.15.0->axolotl)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.15.0->axolotl)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (4.19.2)\n",
      "Collecting toolz (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.2->gcsfs->axolotl)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.2->gcsfs->axolotl)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.2->gcsfs->axolotl)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio==3.50.2->axolotl)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio==3.50.2->axolotl)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio==3.50.2->axolotl)\n",
      "  Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib~=3.0->gradio==3.50.2->axolotl)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==2.15.0->axolotl)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==2.15.0->axolotl)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl) (0.2.9)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl) (2.16.1)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl==0.8.5->axolotl)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.5->axolotl)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.50.2->axolotl)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum==1.16.2->axolotl)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio==3.50.2->axolotl)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->gcsfs->axolotl)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage->gcsfs->axolotl)\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage->gcsfs->axolotl)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage->gcsfs->axolotl)\n",
      "  Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage->gcsfs->axolotl)\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->axolotl) (4.0.0)\n",
      "Collecting httpcore==1.* (from httpx->gradio==3.50.2->axolotl)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->axolotl) (1.3.0)\n",
      "Collecting wavedrom (from markdown2[all]->fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl)\n",
      "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.15.0->axolotl)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum==1.16.2->axolotl) (1.3.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs->axolotl)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->axolotl)\n",
      "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->axolotl)\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (0.10.6)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->axolotl)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->axolotl) (3.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.50.2->axolotl) (1.1.3)\n",
      "Collecting svgwrite (from wavedrom->markdown2[all]->fschat@ git+https://github.com/lm-sys/FastChat.git@5095615810cf613dba7f27dd155f571fcff976d8->axolotl)\n",
      "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading optimum-1.16.2-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.5/402.5 kB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m155.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.8.5-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.1/245.1 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m133.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m163.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Downloading art-6.2-py3-none-any.whl (601 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.8/601.8 kB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastcore-1.5.33-py3-none-any.whl (69 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.3/69.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gcsfs-2023.10.0-py2.py3-none-any.whl (33 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading s3fs-2023.10.0-py3-none-any.whl (28 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m138.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiobotocore-2.7.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m156.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m155.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m161.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.1/774.1 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading sentry_sdk-2.0.1-py2.py3-none-any.whl (266 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading tyro-0.8.3-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nh3-0.2.17-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.1/777.1 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading botocore-1.31.64-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m141.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown2-2.4.13-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.1/229.1 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers, fire, fschat, ffmpy, wavedrom\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.40.0.dev0-py3-none-any.whl size=8796805 sha256=de015cd08f062c3e0a5cd4eafcaaba99147ed5601650fcaa1ca350022b163120\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/f5/ed/f412b48578ab0118dabcef85168b0d6ee86b449fcb1a0bd9c3\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=bb751be786c9197ec3e3256f6a2c72c0b90ca8502402f31cb9b9024cfd10885f\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
      "  Building wheel for fschat (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fschat: filename=fschat-0.2.36-py3-none-any.whl size=271663 sha256=d4a64d3591be67278dbd9afaa7e07ddc73e21148f0a75617faee396fbd2a776c\n",
      "  Stored in directory: /root/.cache/pip/wheels/3c/69/91/3a3370778b4102435e22cd3877ff6096b73f29ecdaca902722\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=994cea6ba775571470c341aa5ccb7a43be3e1c72b7c5e09f1d98c00b5708c2fd\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
      "  Building wheel for wavedrom (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30053 sha256=0e29959e9a8252a7bb33ad00857e16a9d4615d95b4d14d0f33f977ec5e82a952\n",
      "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
      "Successfully built transformers fire fschat ffmpy wavedrom\n",
      "Installing collected packages: sentencepiece, pytz, pydub, nh3, ffmpy, appdirs, addict, zstandard, xxhash, wrapt, werkzeug, websockets, tzdata, typing-extensions, tqdm, toolz, threadpoolctl, termcolor, tensorboard-data-server, svgwrite, smmap, shtab, shortuuid, setproctitle, sentry-sdk, semantic-version, safetensors, regex, python-multipart, pynvml, pyasn1, pyarrow-hotfix, protobuf, orjson, numpy, multidict, mdurl, markdown2, markdown, llvmlite, kiwisolver, joblib, jmespath, importlib-resources, humanfriendly, hf_transfer, h11, grpcio, google-crc32c, fsspec, frozenlist, fonttools, fastcore, einops, docstring-parser, docker-pycreds, dill, cycler, colorama, Click, cachetools, async-timeout, art, annotated-types, aioitertools, aiofiles, absl-py, yarl, wavedrom, uvicorn, tiktoken, tensorboard, starlette, scipy, rsa, responses, requests-oauthlib, pydantic-core, pyasn1-modules, pyarrow, proto-plus, pandas, numba, multiprocess, markdown-it-py, huggingface-hub, httpcore, googleapis-common-protos, google-resumable-media, gitdb, fire, contourpy, coloredlogs, botocore, aiosignal, tokenizers, scikit-learn, rich, pydantic, matplotlib, httpx, google-auth, GitPython, aiohttp, xformers, wandb, tyro, transformers, gradio-client, google-auth-oauthlib, google-api-core, fastapi, bitsandbytes, altair, aiobotocore, accelerate, s3fs, peft, gradio, google-cloud-core, fschat, datasets, trl, optimum, google-cloud-storage, evaluate, gcsfs, axolotl\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "  Running setup.py develop for axolotl\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.1.2 which is incompatible.\n",
      "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Click-8.1.7 GitPython-3.1.43 absl-py-2.1.0 accelerate-0.28.0 addict-2.4.0 aiobotocore-2.7.0 aiofiles-23.2.1 aiohttp-3.9.5 aioitertools-0.11.0 aiosignal-1.3.1 altair-5.3.0 annotated-types-0.6.0 appdirs-1.4.4 art-6.2 async-timeout-4.0.3 axolotl bitsandbytes-0.43.0 botocore-1.31.64 cachetools-5.3.3 colorama-0.4.6 coloredlogs-15.0.1 contourpy-1.2.1 cycler-0.12.1 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 docstring-parser-0.16 einops-0.8.0 evaluate-0.4.1 fastapi-0.110.3 fastcore-1.5.33 ffmpy-0.3.2 fire-0.6.0 fonttools-4.51.0 frozenlist-1.4.1 fschat-0.2.36 fsspec-2023.10.0 gcsfs-2023.10.0 gitdb-4.0.11 google-api-core-2.19.0 google-auth-2.29.0 google-auth-oauthlib-1.2.0 google-cloud-core-2.4.1 google-cloud-storage-2.16.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 googleapis-common-protos-1.63.0 gradio-3.50.2 gradio-client-0.6.1 grpcio-1.63.0 h11-0.14.0 hf_transfer-0.1.6 httpcore-1.0.5 httpx-0.27.0 huggingface-hub-0.23.0 humanfriendly-10.0 importlib-resources-6.4.0 jmespath-1.0.1 joblib-1.4.2 kiwisolver-1.4.5 llvmlite-0.42.0 markdown-3.6 markdown-it-py-3.0.0 markdown2-2.4.13 matplotlib-3.8.4 mdurl-0.1.2 multidict-6.0.5 multiprocess-0.70.15 nh3-0.2.17 numba-0.59.1 numpy-1.26.4 optimum-1.16.2 orjson-3.10.2 pandas-2.2.2 peft-0.10.0 proto-plus-1.23.0 protobuf-4.25.3 pyarrow-16.0.0 pyarrow-hotfix-0.6 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.6.3 pydantic-core-2.16.3 pydub-0.25.1 pynvml-11.5.0 python-multipart-0.0.9 pytz-2024.1 regex-2024.4.28 requests-oauthlib-2.0.0 responses-0.18.0 rich-13.7.1 rsa-4.9 s3fs-2023.10.0 safetensors-0.4.3 scikit-learn-1.2.2 scipy-1.13.0 semantic-version-2.10.0 sentencepiece-0.2.0 sentry-sdk-2.0.1 setproctitle-1.3.3 shortuuid-1.0.13 shtab-1.7.1 smmap-5.0.1 starlette-0.37.2 svgwrite-1.4.3 tensorboard-2.16.2 tensorboard-data-server-0.7.2 termcolor-2.4.0 threadpoolctl-3.5.0 tiktoken-0.6.0 tokenizers-0.15.0 toolz-0.12.1 tqdm-4.66.2 transformers-4.40.0.dev0 trl-0.8.5 typing-extensions-4.11.0 tyro-0.8.3 tzdata-2024.1 uvicorn-0.29.0 wandb-0.16.6 wavedrom-2.0.3.post3 websockets-11.0.3 werkzeug-3.0.2 wrapt-1.16.0 xformers-0.0.23.post1 xxhash-3.4.1 yarl-1.9.4 zstandard-0.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting flash-attn==2.5.0\n",
      "  Downloading flash_attn-2.5.0.tar.gz (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.5.0) (2.1.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.5.0) (0.8.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.5.0) (23.2)\n",
      "Collecting ninja (from flash-attn==2.5.0)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn==2.5.0) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn==2.5.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn==2.5.0) (1.3.0)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.0-cp310-cp310-linux_x86_64.whl size=120823033 sha256=3335e74258645eb190597754d42c2fee391fbdeb772847f9e1de12da60450a33\n",
      "  Stored in directory: /root/.cache/pip/wheels/9e/c3/22/a576eb5627fb2c30dc4679a33d67d34d922d6dbeb24a9119b2\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: ninja, flash-attn\n",
      "Successfully installed flash-attn-2.5.0 ninja-1.11.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting deepspeed==0.13.1\n",
      "  Downloading deepspeed-0.13.1.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson (from deepspeed==0.13.1)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed==0.13.1)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1) (2.6.3)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1) (11.5.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1) (2.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1) (4.66.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.13.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.13.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.13.1) (4.11.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed==0.13.1) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed==0.13.1) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.13.1) (1.3.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.13.1-py3-none-any.whl size=1350303 sha256=0c2bc5f14a983e509db4e22c7212853d6d7f695738b74f6f9d849e2fac5c9928\n",
      "  Stored in directory: /root/.cache/pip/wheels/0f/fb/b5/b159b3500525eca167d8ca6e3a7e224b6075045cac90f47cf7\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, hjson, deepspeed\n",
      "Successfully installed deepspeed-0.13.1 hjson-3.1.0 py-cpuinfo-9.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.40.0.dev0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.28.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.4.28)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==\"2.1.2\"\n",
    "!pip install -e git+https://github.com/OpenAccess-AI-Collective/axolotl#egg=axolotl\n",
    "!pip install flash-attn==\"2.5.0\"\n",
    "!pip install deepspeed==\"0.13.1\"\n",
    "!pip install peft --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "109d3f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "auth_token = \"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\"\n",
    "login(token=auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0664fd-d23f-4016-967c-a31e3e1e0eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2024-05-02 13:55:59,654] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-02 13:56:01,310] [DEBUG] [axolotl.normalize_config:79] [PID:1827] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "config.json: 100%|█████████████████████████████| 596/596 [00:00<00:00, 2.58MB/s]\n",
      "[2024-05-02 13:56:01,909] [INFO] [axolotl.normalize_config:182] [PID:1827] [RANK:0] GPU memory usage baseline: 0.000GB (+0.456GB misc)\u001b[39m\n",
      "                                 dP            dP   dP \n",
      "                                 88            88   88 \n",
      "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
      "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
      "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
      "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
      "                                                       \n",
      "                                                       \n",
      "\n",
      "****************************************\n",
      "**** Axolotl Dependency Versions *****\n",
      "  accelerate: 0.28.0         \n",
      "        peft: 0.10.0         \n",
      "transformers: 4.40.0.dev0    \n",
      "         trl: 0.8.5          \n",
      "       torch: 2.1.2          \n",
      "bitsandbytes: 0.43.0         \n",
      "****************************************\n",
      "tokenizer_config.json: 100%|███████████████| 1.46k/1.46k [00:00<00:00, 17.7MB/s]\n",
      "tokenizer.model: 100%|███████████████████████| 493k/493k [00:00<00:00, 11.3MB/s]\n",
      "special_tokens_map.json: 100%|████████████████| 72.0/72.0 [00:00<00:00, 453kB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.80M/1.80M [00:00<00:00, 5.55MB/s]\n",
      "[2024-05-02 13:56:03,967] [DEBUG] [axolotl.load_tokenizer:279] [PID:1827] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2024-05-02 13:56:03,967] [DEBUG] [axolotl.load_tokenizer:280] [PID:1827] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2024-05-02 13:56:03,967] [DEBUG] [axolotl.load_tokenizer:281] [PID:1827] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2024-05-02 13:56:03,967] [DEBUG] [axolotl.load_tokenizer:282] [PID:1827] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2024-05-02 13:56:03,967] [INFO] [axolotl.load_tokenizer:293] [PID:1827] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-05-02 13:56:03,968] [INFO] [axolotl.load_tokenized_prepared_datasets:183] [PID:1827] [RANK:0] Unable to find prepared dataset in last_run_prepared/b561fe7d262c670d9d2303163fc63b06\u001b[39m\n",
      "[2024-05-02 13:56:03,968] [INFO] [axolotl.load_tokenized_prepared_datasets:184] [PID:1827] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2024-05-02 13:56:03,968] [WARNING] [axolotl.load_tokenized_prepared_datasets:186] [PID:1827] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
      "[2024-05-02 13:56:03,968] [INFO] [axolotl.load_tokenized_prepared_datasets:193] [PID:1827] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 1164.11it/s]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:00<00:00, 92.53it/s]\n",
      "Generating train split: 34 examples [00:00, 3105.20 examples/s]\n",
      "num_proc must be <= 34. Reducing num_proc to 34 for dataset of size 34.\n",
      "[2024-05-02 13:56:04,575] [WARNING] [datasets.arrow_dataset.map:3019] [PID:1827] num_proc must be <= 34. Reducing num_proc to 34 for dataset of size 34.\n",
      "Tokenizing Prompts (num_proc=34): 100%|█| 34/34 [00:00<00:00, 108.19 examples/s]\n",
      "[2024-05-02 13:56:05,325] [INFO] [axolotl.load_tokenized_prepared_datasets:410] [PID:1827] [RANK:0] merging datasets\u001b[39m\n",
      "[2024-05-02 13:56:05,328] [INFO] [axolotl.log:61] [PID:1827] [RANK:0] dropping attention_mask column\u001b[39m\n",
      "num_proc must be <= 34. Reducing num_proc to 34 for dataset of size 34.\n",
      "[2024-05-02 13:56:05,334] [WARNING] [datasets.arrow_dataset.map:3019] [PID:1827] num_proc must be <= 34. Reducing num_proc to 34 for dataset of size 34.\n",
      "Dropping Long Sequences (num_proc=34): 100%|█| 34/34 [00:00<00:00, 127.69 exampl\n",
      "num_proc must be <= 34. Reducing num_proc to 34 for dataset of size 34.\n",
      "[2024-05-02 13:56:06,115] [WARNING] [datasets.arrow_dataset.map:3019] [PID:1827] num_proc must be <= 34. Reducing num_proc to 34 for dataset of size 34.\n",
      "Add position_id column (Sample Packing) (num_proc=34): 100%|█| 34/34 [00:00<00:0\n",
      "[2024-05-02 13:56:06,925] [INFO] [axolotl.load_tokenized_prepared_datasets:423] [PID:1827] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/b561fe7d262c670d9d2303163fc63b06\u001b[39m\n",
      "Saving the dataset (1/1 shards): 100%|██| 34/34 [00:00<00:00, 645.94 examples/s]\n",
      "[2024-05-02 13:56:07,040] [DEBUG] [axolotl.log:61] [PID:1827] [RANK:0] total_num_tokens: 58_383\u001b[39m\n",
      "[2024-05-02 13:56:07,040] [DEBUG] [axolotl.log:61] [PID:1827] [RANK:0] `total_supervised_tokens: 43_525`\u001b[39m\n",
      "[2024-05-02 13:56:10,465] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 58383\u001b[39m\n",
      "[2024-05-02 13:56:10,465] [DEBUG] [axolotl.log:61] [PID:1827] [RANK:0] data_loader_len: 0\u001b[39m\n",
      "[2024-05-02 13:56:10,465] [INFO] [axolotl.log:61] [PID:1827] [RANK:0] sample_packing_eff_est across ranks: [0.8908538818359375]\u001b[39m\n",
      "[2024-05-02 13:56:10,465] [DEBUG] [axolotl.log:61] [PID:1827] [RANK:0] sample_packing_eff_est: 0.9\u001b[39m\n",
      "[2024-05-02 13:56:10,465] [DEBUG] [axolotl.log:61] [PID:1827] [RANK:0] total_num_steps: 0\u001b[39m\n",
      "[2024-05-02 13:56:10,470] [DEBUG] [axolotl.train.log:61] [PID:1827] [RANK:0] loading tokenizer... mistralai/Mistral-7B-Instruct-v0.2\u001b[39m\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 13:56:10,811] [DEBUG] [axolotl.load_tokenizer:279] [PID:1827] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2024-05-02 13:56:10,811] [DEBUG] [axolotl.load_tokenizer:280] [PID:1827] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2024-05-02 13:56:10,811] [DEBUG] [axolotl.load_tokenizer:281] [PID:1827] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2024-05-02 13:56:10,811] [DEBUG] [axolotl.load_tokenizer:282] [PID:1827] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2024-05-02 13:56:10,811] [INFO] [axolotl.load_tokenizer:293] [PID:1827] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-05-02 13:56:10,811] [DEBUG] [axolotl.train.log:61] [PID:1827] [RANK:0] loading model and peft_config...\u001b[39m\n",
      "[2024-05-02 13:56:10,819] [WARNING] [accelerate.utils.other.log:61] [PID:1827] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2024-05-02 13:56:11,032] [INFO] [axolotl.load_model:402] [PID:1827] [RANK:0] patching mistral with flash attention\u001b[39m\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "model.safetensors.index.json: 100%|████████| 25.1k/25.1k [00:00<00:00, 87.5MB/s]\n",
      "Downloading shards:   0%|                                 | 0/3 [00:00<?, ?it/s]\n",
      "model-00001-of-00003.safetensors:   0%|             | 0.00/4.94G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   0%|    | 10.5M/4.94G [00:00<02:25, 34.0MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   0%|    | 21.0M/4.94G [00:00<01:43, 47.7MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   1%|    | 31.5M/4.94G [00:00<01:21, 60.6MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   1%|    | 52.4M/4.94G [00:00<00:56, 87.1MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   1%|     | 73.4M/4.94G [00:00<00:42, 114MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   2%|▏     | 105M/4.94G [00:00<00:31, 154MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   3%|▏     | 126M/4.94G [00:01<00:42, 114MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   3%|▏     | 157M/4.94G [00:01<00:34, 137MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   4%|▏     | 189M/4.94G [00:01<00:28, 166MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   4%|▎     | 220M/4.94G [00:01<00:28, 165MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   5%|▎     | 241M/4.94G [00:01<00:29, 159MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   5%|▎     | 262M/4.94G [00:02<00:31, 150MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   6%|▎     | 294M/4.94G [00:02<00:26, 175MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   6%|▍     | 315M/4.94G [00:02<00:26, 173MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   7%|▍     | 336M/4.94G [00:02<00:30, 150MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   7%|▍     | 367M/4.94G [00:02<00:26, 173MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   8%|▍     | 398M/4.94G [00:02<00:24, 189MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   9%|▌     | 430M/4.94G [00:02<00:22, 201MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   9%|▌     | 461M/4.94G [00:03<00:20, 219MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  10%|▌     | 493M/4.94G [00:03<00:19, 234MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  11%|▋     | 524M/4.94G [00:03<00:18, 235MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  11%|▋     | 556M/4.94G [00:03<00:17, 248MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  12%|▋     | 587M/4.94G [00:03<00:16, 257MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  13%|▊     | 619M/4.94G [00:03<00:17, 251MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  13%|▊     | 650M/4.94G [00:03<00:16, 254MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  14%|▊     | 682M/4.94G [00:03<00:16, 261MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  14%|▊     | 713M/4.94G [00:03<00:16, 255MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  15%|▉     | 744M/4.94G [00:04<00:15, 263MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  16%|▉     | 776M/4.94G [00:04<00:15, 262MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  16%|▉     | 807M/4.94G [00:04<00:15, 262MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  17%|█     | 839M/4.94G [00:04<00:15, 265MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  18%|█     | 870M/4.94G [00:04<00:15, 260MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  18%|█     | 902M/4.94G [00:04<00:15, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  19%|█▏    | 933M/4.94G [00:04<00:15, 259MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  20%|█▏    | 965M/4.94G [00:04<00:15, 257MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  20%|█▏    | 996M/4.94G [00:05<00:15, 252MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  21%|█    | 1.03G/4.94G [00:05<00:15, 251MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  21%|█    | 1.06G/4.94G [00:05<00:15, 244MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  22%|█    | 1.09G/4.94G [00:05<00:15, 250MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  23%|█▏   | 1.12G/4.94G [00:05<00:14, 257MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  23%|█▏   | 1.15G/4.94G [00:05<00:14, 254MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  24%|█▏   | 1.18G/4.94G [00:05<00:15, 249MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  25%|█▏   | 1.22G/4.94G [00:05<00:14, 256MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  25%|█▎   | 1.25G/4.94G [00:06<00:14, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  26%|█▎   | 1.28G/4.94G [00:06<00:14, 248MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  27%|█▎   | 1.31G/4.94G [00:06<00:14, 244MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  27%|█▎   | 1.34G/4.94G [00:06<00:14, 242MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  28%|█▍   | 1.37G/4.94G [00:06<00:14, 249MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  28%|█▍   | 1.41G/4.94G [00:06<00:14, 250MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  29%|█▍   | 1.44G/4.94G [00:06<00:14, 249MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  30%|█▍   | 1.47G/4.94G [00:06<00:14, 240MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  30%|█▌   | 1.50G/4.94G [00:07<00:15, 219MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  31%|█▌   | 1.53G/4.94G [00:07<00:14, 228MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  32%|█▌   | 1.56G/4.94G [00:07<00:14, 233MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  32%|█▌   | 1.59G/4.94G [00:07<00:13, 245MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  33%|█▋   | 1.63G/4.94G [00:07<00:13, 241MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  34%|█▋   | 1.66G/4.94G [00:07<00:13, 238MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  34%|█▋   | 1.69G/4.94G [00:07<00:13, 250MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  35%|█▋   | 1.72G/4.94G [00:08<00:12, 249MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  35%|█▊   | 1.75G/4.94G [00:08<00:12, 251MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  36%|█▊   | 1.78G/4.94G [00:08<00:12, 257MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  37%|█▊   | 1.81G/4.94G [00:08<00:12, 252MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  37%|█▊   | 1.85G/4.94G [00:08<00:12, 249MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  38%|█▉   | 1.88G/4.94G [00:08<00:11, 259MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  39%|█▉   | 1.91G/4.94G [00:08<00:12, 250MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  39%|█▉   | 1.94G/4.94G [00:08<00:12, 245MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  40%|█▉   | 1.97G/4.94G [00:09<00:11, 255MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  41%|██   | 2.00G/4.94G [00:09<00:11, 258MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  41%|██   | 2.03G/4.94G [00:09<00:11, 251MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  42%|██   | 2.07G/4.94G [00:09<00:11, 249MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  42%|██   | 2.10G/4.94G [00:09<00:12, 232MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  43%|██▏  | 2.13G/4.94G [00:09<00:12, 234MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  44%|██▏  | 2.16G/4.94G [00:09<00:11, 246MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  44%|██▏  | 2.19G/4.94G [00:09<00:11, 243MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  45%|██▏  | 2.22G/4.94G [00:10<00:15, 180MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  45%|██▎  | 2.24G/4.94G [00:10<00:18, 146MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  46%|██▎  | 2.26G/4.94G [00:10<00:26, 102MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  46%|█▊  | 2.29G/4.94G [00:11<00:27, 97.2MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  47%|██▎  | 2.31G/4.94G [00:11<00:24, 106MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  47%|█▉  | 2.33G/4.94G [00:11<00:30, 85.8MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  48%|█▉  | 2.35G/4.94G [00:11<00:27, 92.7MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  48%|█▉  | 2.37G/4.94G [00:12<00:32, 78.6MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  48%|█▉  | 2.38G/4.94G [00:12<00:35, 71.2MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  49%|█▉  | 2.40G/4.94G [00:12<00:31, 80.6MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  49%|█▉  | 2.41G/4.94G [00:12<00:33, 75.0MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  49%|█▉  | 2.42G/4.94G [00:12<00:31, 79.2MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  49%|█▉  | 2.43G/4.94G [00:13<00:34, 72.5MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  50%|█▉  | 2.45G/4.94G [00:13<00:29, 83.7MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  50%|██  | 2.47G/4.94G [00:13<00:30, 81.9MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  50%|██  | 2.49G/4.94G [00:13<00:32, 75.4MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  50%|██  | 2.50G/4.94G [00:13<00:30, 79.1MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  51%|██  | 2.52G/4.94G [00:14<00:32, 74.5MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  51%|██  | 2.53G/4.94G [00:14<00:30, 78.4MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  52%|██  | 2.55G/4.94G [00:14<00:28, 82.8MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  52%|██  | 2.56G/4.94G [00:14<00:33, 70.4MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  52%|██  | 2.58G/4.94G [00:14<00:30, 78.3MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  53%|██  | 2.60G/4.94G [00:15<00:24, 97.5MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  53%|██▋  | 2.63G/4.94G [00:15<00:19, 118MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  54%|██▋  | 2.65G/4.94G [00:15<00:20, 111MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  54%|██▋  | 2.67G/4.94G [00:15<00:18, 120MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  55%|██▋  | 2.71G/4.94G [00:15<00:14, 154MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  55%|██▊  | 2.74G/4.94G [00:15<00:12, 180MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  56%|██▊  | 2.77G/4.94G [00:15<00:10, 201MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  57%|██▊  | 2.80G/4.94G [00:16<00:09, 221MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  57%|██▊  | 2.83G/4.94G [00:16<00:09, 221MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  58%|██▉  | 2.86G/4.94G [00:16<00:09, 231MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  59%|██▉  | 2.89G/4.94G [00:16<00:08, 240MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  59%|██▉  | 2.93G/4.94G [00:16<00:08, 242MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  60%|██▉  | 2.96G/4.94G [00:16<00:08, 230MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  60%|███  | 2.99G/4.94G [00:16<00:08, 237MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  61%|███  | 3.02G/4.94G [00:16<00:08, 237MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  62%|███  | 3.05G/4.94G [00:17<00:07, 244MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  62%|███  | 3.08G/4.94G [00:17<00:07, 252MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  63%|███▏ | 3.11G/4.94G [00:17<00:07, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  64%|███▏ | 3.15G/4.94G [00:17<00:07, 256MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  64%|███▏ | 3.18G/4.94G [00:17<00:07, 238MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  65%|███▏ | 3.21G/4.94G [00:17<00:07, 236MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  66%|███▎ | 3.24G/4.94G [00:17<00:07, 239MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  66%|███▎ | 3.27G/4.94G [00:17<00:06, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  67%|███▎ | 3.30G/4.94G [00:18<00:07, 220MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  67%|███▎ | 3.33G/4.94G [00:18<00:06, 232MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  68%|███▍ | 3.37G/4.94G [00:18<00:06, 246MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  69%|███▍ | 3.40G/4.94G [00:18<00:06, 225MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  69%|███▍ | 3.43G/4.94G [00:18<00:06, 238MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  70%|███▌ | 3.46G/4.94G [00:18<00:06, 245MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  71%|███▌ | 3.49G/4.94G [00:18<00:06, 240MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  71%|███▌ | 3.52G/4.94G [00:19<00:05, 245MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  72%|███▌ | 3.55G/4.94G [00:19<00:05, 252MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  73%|███▋ | 3.59G/4.94G [00:19<00:05, 248MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  73%|███▋ | 3.62G/4.94G [00:19<00:05, 251MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  74%|███▋ | 3.65G/4.94G [00:19<00:05, 257MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  74%|███▋ | 3.68G/4.94G [00:19<00:05, 252MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  75%|███▊ | 3.71G/4.94G [00:19<00:04, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  76%|███▊ | 3.74G/4.94G [00:19<00:04, 258MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  76%|███▊ | 3.77G/4.94G [00:20<00:04, 252MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  77%|███▊ | 3.81G/4.94G [00:20<00:04, 261MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  78%|███▉ | 3.84G/4.94G [00:20<00:04, 261MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  78%|███▉ | 3.87G/4.94G [00:20<00:04, 256MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  79%|███▉ | 3.90G/4.94G [00:20<00:03, 263MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  80%|███▉ | 3.93G/4.94G [00:20<00:03, 265MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  80%|████ | 3.96G/4.94G [00:20<00:03, 255MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  81%|████ | 4.00G/4.94G [00:20<00:03, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  81%|████ | 4.03G/4.94G [00:20<00:03, 251MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  82%|████ | 4.06G/4.94G [00:21<00:03, 248MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  83%|████▏| 4.09G/4.94G [00:21<00:03, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  83%|████▏| 4.12G/4.94G [00:21<00:03, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  84%|████▏| 4.15G/4.94G [00:21<00:03, 241MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  85%|████▏| 4.18G/4.94G [00:21<00:03, 234MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  85%|████▎| 4.22G/4.94G [00:21<00:02, 245MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  86%|████▎| 4.25G/4.94G [00:21<00:02, 243MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  87%|████▎| 4.28G/4.94G [00:22<00:02, 254MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  87%|████▎| 4.31G/4.94G [00:22<00:02, 251MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  88%|████▍| 4.34G/4.94G [00:22<00:02, 246MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  88%|████▍| 4.37G/4.94G [00:22<00:02, 256MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  89%|████▍| 4.40G/4.94G [00:22<00:02, 256MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  90%|████▍| 4.44G/4.94G [00:22<00:02, 242MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  90%|████▌| 4.47G/4.94G [00:22<00:01, 254MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  91%|████▌| 4.50G/4.94G [00:22<00:01, 256MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  92%|████▌| 4.53G/4.94G [00:23<00:01, 249MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  92%|████▌| 4.56G/4.94G [00:23<00:01, 259MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  93%|████▋| 4.59G/4.94G [00:23<00:01, 255MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  94%|████▋| 4.62G/4.94G [00:23<00:01, 255MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  94%|████▋| 4.66G/4.94G [00:23<00:01, 263MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  95%|████▋| 4.69G/4.94G [00:23<00:00, 257MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  95%|████▊| 4.72G/4.94G [00:23<00:00, 247MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  96%|████▊| 4.75G/4.94G [00:23<00:00, 257MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  97%|████▊| 4.78G/4.94G [00:24<00:00, 252MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  97%|████▊| 4.81G/4.94G [00:24<00:00, 251MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  98%|████▉| 4.84G/4.94G [00:24<00:00, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  99%|████▉| 4.88G/4.94G [00:24<00:00, 164MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  99%|████▉| 4.90G/4.94G [00:24<00:00, 172MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors: 100%|█████| 4.94G/4.94G [00:24<00:00, 199MB/s]\u001b[A\n",
      "Downloading shards:  33%|████████▎                | 1/3 [00:25<00:50, 25.13s/it]\n",
      "model-00002-of-00003.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   0%|    | 10.5M/5.00G [00:00<03:13, 25.8MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   0%|    | 21.0M/5.00G [00:00<02:14, 37.1MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   1%|    | 31.5M/5.00G [00:00<01:42, 48.6MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   1%|    | 41.9M/5.00G [00:00<01:21, 60.9MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   1%|    | 62.9M/5.00G [00:00<00:55, 88.2MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   2%|     | 83.9M/5.00G [00:01<00:42, 115MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   2%|▏     | 115M/5.00G [00:01<00:32, 150MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   3%|▏     | 147M/5.00G [00:01<00:27, 175MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   4%|▏     | 178M/5.00G [00:01<00:23, 201MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   4%|▎     | 210M/5.00G [00:01<00:23, 202MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   5%|▎     | 231M/5.00G [00:01<00:24, 198MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   5%|▎     | 262M/5.00G [00:01<00:21, 220MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   6%|▎     | 294M/5.00G [00:02<00:21, 224MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   7%|▍     | 325M/5.00G [00:02<00:21, 220MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   7%|▍     | 357M/5.00G [00:02<00:19, 233MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   8%|▍     | 388M/5.00G [00:02<00:20, 226MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   8%|▌     | 419M/5.00G [00:02<00:19, 239MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   9%|▌     | 451M/5.00G [00:02<00:18, 252MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  10%|▌     | 482M/5.00G [00:02<00:17, 254MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  10%|▌     | 514M/5.00G [00:02<00:17, 257MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  11%|▋     | 545M/5.00G [00:03<00:17, 257MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  12%|▋     | 577M/5.00G [00:03<00:17, 255MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  12%|▋     | 608M/5.00G [00:03<00:17, 253MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  13%|▊     | 640M/5.00G [00:03<00:16, 263MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  13%|▊     | 671M/5.00G [00:03<00:16, 255MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  14%|▊     | 703M/5.00G [00:03<00:16, 259MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  15%|▉     | 734M/5.00G [00:03<00:16, 266MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  15%|▉     | 765M/5.00G [00:03<00:15, 265MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  16%|▉     | 797M/5.00G [00:03<00:16, 255MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  17%|▉     | 828M/5.00G [00:04<00:16, 258MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  17%|█     | 860M/5.00G [00:04<00:16, 254MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  18%|█     | 891M/5.00G [00:04<00:17, 240MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  18%|█     | 923M/5.00G [00:04<00:16, 251MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  19%|█▏    | 954M/5.00G [00:04<00:16, 247MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  20%|█▏    | 986M/5.00G [00:04<00:17, 233MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  20%|█    | 1.02G/5.00G [00:04<00:16, 245MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  21%|█    | 1.05G/5.00G [00:05<00:16, 235MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  22%|█    | 1.08G/5.00G [00:05<00:16, 237MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  22%|█    | 1.11G/5.00G [00:05<00:15, 249MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  23%|█▏   | 1.14G/5.00G [00:05<00:15, 254MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  23%|█▏   | 1.17G/5.00G [00:05<00:14, 257MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  24%|█▏   | 1.21G/5.00G [00:05<00:14, 261MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  25%|█▏   | 1.24G/5.00G [00:05<00:14, 252MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  25%|█▎   | 1.27G/5.00G [00:05<00:15, 240MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  26%|█▎   | 1.30G/5.00G [00:06<00:14, 251MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  27%|█▎   | 1.33G/5.00G [00:06<00:14, 246MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  27%|█▎   | 1.36G/5.00G [00:06<00:15, 239MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  28%|█▍   | 1.39G/5.00G [00:06<00:14, 251MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  29%|█▍   | 1.43G/5.00G [00:06<00:15, 237MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  29%|█▍   | 1.46G/5.00G [00:06<00:16, 216MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  30%|█▍   | 1.49G/5.00G [00:06<00:15, 233MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  30%|█▌   | 1.52G/5.00G [00:06<00:14, 241MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  31%|█▌   | 1.55G/5.00G [00:07<00:13, 249MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  32%|█▌   | 1.58G/5.00G [00:07<00:14, 241MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  32%|█▌   | 1.61G/5.00G [00:07<00:14, 237MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  33%|█▋   | 1.65G/5.00G [00:07<00:13, 245MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  34%|█▋   | 1.68G/5.00G [00:07<00:13, 254MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  34%|█▋   | 1.71G/5.00G [00:07<00:13, 245MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  35%|█▋   | 1.74G/5.00G [00:07<00:13, 240MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  35%|█▊   | 1.77G/5.00G [00:07<00:12, 250MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  36%|█▊   | 1.80G/5.00G [00:08<00:13, 244MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  37%|█▊   | 1.84G/5.00G [00:08<00:12, 255MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  37%|█▊   | 1.87G/5.00G [00:08<00:12, 260MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  38%|█▉   | 1.90G/5.00G [00:08<00:12, 249MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  39%|█▉   | 1.93G/5.00G [00:08<00:12, 253MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  39%|█▉   | 1.96G/5.00G [00:08<00:11, 260MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  40%|█▉   | 1.99G/5.00G [00:08<00:12, 239MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  40%|██   | 2.02G/5.00G [00:08<00:12, 246MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  41%|██   | 2.06G/5.00G [00:09<00:11, 253MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  42%|██   | 2.09G/5.00G [00:09<00:11, 247MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  42%|██   | 2.12G/5.00G [00:09<00:11, 257MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  43%|██▏  | 2.15G/5.00G [00:09<00:10, 263MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  44%|██▏  | 2.18G/5.00G [00:09<00:16, 167MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  44%|██▏  | 2.21G/5.00G [00:10<00:22, 125MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  45%|██▏  | 2.23G/5.00G [00:10<00:20, 136MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  45%|██▎  | 2.26G/5.00G [00:10<00:17, 158MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  46%|██▎  | 2.30G/5.00G [00:10<00:14, 181MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  47%|██▎  | 2.33G/5.00G [00:10<00:18, 144MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  47%|██▎  | 2.35G/5.00G [00:11<00:22, 119MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  47%|██▎  | 2.37G/5.00G [00:11<00:20, 130MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  48%|██▍  | 2.40G/5.00G [00:11<00:16, 154MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  49%|██▍  | 2.43G/5.00G [00:11<00:15, 164MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  49%|██▍  | 2.46G/5.00G [00:11<00:13, 183MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  50%|██▍  | 2.49G/5.00G [00:11<00:18, 135MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  50%|██▌  | 2.52G/5.00G [00:12<00:15, 164MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  51%|██▌  | 2.55G/5.00G [00:12<00:13, 180MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  52%|██▌  | 2.58G/5.00G [00:12<00:12, 195MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  52%|██▌  | 2.61G/5.00G [00:12<00:11, 208MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  53%|██▋  | 2.64G/5.00G [00:12<00:11, 214MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  53%|██▋  | 2.67G/5.00G [00:12<00:10, 229MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  54%|██▋  | 2.71G/5.00G [00:12<00:09, 231MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  55%|██▋  | 2.74G/5.00G [00:13<00:10, 215MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  55%|██▊  | 2.77G/5.00G [00:13<00:10, 223MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  56%|██▊  | 2.80G/5.00G [00:13<00:09, 230MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  57%|██▊  | 2.83G/5.00G [00:13<00:09, 229MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  57%|██▊  | 2.86G/5.00G [00:13<00:12, 169MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  58%|██▉  | 2.88G/5.00G [00:13<00:11, 176MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  58%|██▉  | 2.90G/5.00G [00:13<00:11, 181MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  59%|██▉  | 2.94G/5.00G [00:14<00:10, 189MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  59%|██▉  | 2.97G/5.00G [00:14<00:09, 206MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  60%|██▉  | 3.00G/5.00G [00:14<00:09, 203MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  61%|███  | 3.03G/5.00G [00:14<00:09, 203MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  61%|███  | 3.06G/5.00G [00:14<00:09, 194MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  62%|███  | 3.08G/5.00G [00:15<00:14, 136MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  62%|███  | 3.10G/5.00G [00:15<00:13, 138MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  63%|███▏ | 3.14G/5.00G [00:15<00:11, 161MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  63%|███▏ | 3.17G/5.00G [00:15<00:10, 181MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  64%|███▏ | 3.19G/5.00G [00:15<00:10, 176MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  64%|███▏ | 3.21G/5.00G [00:15<00:09, 181MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  65%|███▏ | 3.24G/5.00G [00:15<00:08, 206MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  65%|███▎ | 3.27G/5.00G [00:15<00:08, 211MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  66%|███▎ | 3.30G/5.00G [00:16<00:08, 206MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  67%|███▎ | 3.33G/5.00G [00:16<00:07, 228MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  67%|███▎ | 3.37G/5.00G [00:16<00:07, 216MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  68%|███▍ | 3.40G/5.00G [00:16<00:07, 210MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  69%|███▍ | 3.43G/5.00G [00:16<00:06, 228MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  69%|███▍ | 3.46G/5.00G [00:16<00:06, 239MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  70%|███▍ | 3.49G/5.00G [00:16<00:06, 234MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  70%|███▌ | 3.52G/5.00G [00:17<00:05, 247MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  71%|███▌ | 3.55G/5.00G [00:17<00:05, 251MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  72%|███▌ | 3.59G/5.00G [00:17<00:05, 252MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  72%|███▌ | 3.62G/5.00G [00:17<00:05, 262MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  73%|███▋ | 3.65G/5.00G [00:17<00:05, 257MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  74%|███▋ | 3.68G/5.00G [00:17<00:05, 253MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  74%|███▋ | 3.71G/5.00G [00:17<00:04, 260MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  75%|███▋ | 3.74G/5.00G [00:17<00:04, 253MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  76%|███▊ | 3.77G/5.00G [00:17<00:04, 253MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  76%|███▊ | 3.81G/5.00G [00:18<00:04, 255MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  77%|███▊ | 3.84G/5.00G [00:18<00:04, 250MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  77%|███▊ | 3.87G/5.00G [00:18<00:04, 246MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  78%|███▉ | 3.90G/5.00G [00:18<00:04, 255MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  79%|███▉ | 3.93G/5.00G [00:18<00:04, 249MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  79%|███▉ | 3.96G/5.00G [00:18<00:04, 248MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  80%|███▉ | 4.00G/5.00G [00:18<00:04, 248MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  81%|████ | 4.03G/5.00G [00:18<00:03, 252MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  81%|████ | 4.06G/5.00G [00:19<00:03, 258MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  82%|████ | 4.09G/5.00G [00:19<00:03, 260MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  82%|████ | 4.12G/5.00G [00:19<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  83%|████▏| 4.15G/5.00G [00:19<00:03, 260MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  84%|████▏| 4.18G/5.00G [00:19<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  84%|████▏| 4.22G/5.00G [00:19<00:03, 239MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  85%|████▏| 4.25G/5.00G [00:19<00:03, 232MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  86%|████▎| 4.28G/5.00G [00:20<00:02, 246MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  86%|████▎| 4.31G/5.00G [00:20<00:03, 230MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  87%|████▎| 4.34G/5.00G [00:20<00:02, 236MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  87%|████▎| 4.37G/5.00G [00:20<00:02, 247MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  88%|████▍| 4.40G/5.00G [00:20<00:02, 232MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  89%|████▍| 4.44G/5.00G [00:20<00:02, 239MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  89%|████▍| 4.47G/5.00G [00:20<00:02, 253MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  90%|████▍| 4.50G/5.00G [00:20<00:01, 261MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  91%|████▌| 4.53G/5.00G [00:21<00:01, 264MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  91%|████▌| 4.56G/5.00G [00:21<00:01, 260MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  92%|████▌| 4.59G/5.00G [00:21<00:01, 251MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  92%|████▌| 4.62G/5.00G [00:21<00:01, 256MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  93%|████▋| 4.66G/5.00G [00:21<00:01, 262MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  94%|████▋| 4.69G/5.00G [00:21<00:01, 243MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  94%|████▋| 4.72G/5.00G [00:21<00:01, 251MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  95%|████▊| 4.75G/5.00G [00:21<00:00, 257MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  96%|████▊| 4.78G/5.00G [00:22<00:00, 251MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  96%|████▊| 4.81G/5.00G [00:22<00:00, 261MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  97%|████▊| 4.84G/5.00G [00:22<00:00, 242MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  98%|████▉| 4.88G/5.00G [00:22<00:00, 155MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  98%|████▉| 4.91G/5.00G [00:22<00:00, 174MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  99%|████▉| 4.94G/5.00G [00:22<00:00, 195MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors: 100%|█████| 5.00G/5.00G [00:23<00:00, 216MB/s]\u001b[A\n",
      "Downloading shards:  67%|████████████████▋        | 2/3 [00:48<00:24, 24.09s/it]\n",
      "model-00003-of-00003.safetensors:   0%|             | 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   0%|    | 10.5M/4.54G [00:00<01:34, 47.9MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   0%|    | 21.0M/4.54G [00:00<01:14, 60.6MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   1%|    | 31.5M/4.54G [00:00<01:01, 73.8MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   1%|    | 52.4M/4.54G [00:00<00:45, 98.0MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   2%|     | 73.4M/4.54G [00:00<00:36, 124MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   2%|▏     | 105M/4.54G [00:00<00:28, 158MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   3%|▏     | 136M/4.54G [00:01<00:25, 175MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   4%|▏     | 168M/4.54G [00:01<00:21, 200MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   4%|▎     | 199M/4.54G [00:01<00:19, 221MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   5%|▎     | 231M/4.54G [00:01<00:18, 231MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   6%|▎     | 262M/4.54G [00:01<00:18, 226MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   6%|▍     | 294M/4.54G [00:01<00:18, 229MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   7%|▍     | 325M/4.54G [00:01<00:18, 228MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   8%|▍     | 357M/4.54G [00:01<00:18, 227MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   9%|▌     | 388M/4.54G [00:02<00:17, 243MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   9%|▌     | 419M/4.54G [00:02<00:17, 242MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  10%|▌     | 451M/4.54G [00:02<00:16, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  11%|▋     | 482M/4.54G [00:02<00:16, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  11%|▋     | 514M/4.54G [00:02<00:16, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  12%|▋     | 545M/4.54G [00:02<00:15, 252MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  13%|▊     | 577M/4.54G [00:02<00:15, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  13%|▊     | 608M/4.54G [00:02<00:15, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  14%|▊     | 640M/4.54G [00:03<00:16, 238MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  15%|▉     | 671M/4.54G [00:03<00:17, 224MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  15%|▉     | 703M/4.54G [00:03<00:17, 221MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  16%|▉     | 734M/4.54G [00:03<00:16, 231MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  17%|█     | 765M/4.54G [00:03<00:15, 239MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  18%|█     | 797M/4.54G [00:03<00:16, 232MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  18%|█     | 828M/4.54G [00:03<00:15, 238MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  19%|█▏    | 860M/4.54G [00:04<00:15, 240MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  20%|█▏    | 891M/4.54G [00:04<00:15, 234MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  20%|█▏    | 923M/4.54G [00:04<00:14, 242MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  21%|█▎    | 954M/4.54G [00:04<00:15, 238MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  22%|█▎    | 986M/4.54G [00:04<00:15, 231MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  22%|█    | 1.02G/4.54G [00:04<00:14, 244MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  23%|█▏   | 1.05G/4.54G [00:04<00:13, 252MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  24%|█▏   | 1.08G/4.54G [00:04<00:13, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  24%|█▏   | 1.11G/4.54G [00:05<00:13, 256MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  25%|█▎   | 1.14G/4.54G [00:05<00:13, 245MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  26%|█▎   | 1.17G/4.54G [00:05<00:14, 238MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  27%|█▎   | 1.21G/4.54G [00:05<00:13, 247MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  27%|█▎   | 1.24G/4.54G [00:05<00:13, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  28%|█▍   | 1.27G/4.54G [00:05<00:13, 243MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  29%|█▍   | 1.30G/4.54G [00:05<00:12, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  29%|█▍   | 1.33G/4.54G [00:05<00:12, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  30%|█▌   | 1.36G/4.54G [00:06<00:13, 242MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  31%|█▌   | 1.39G/4.54G [00:06<00:12, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  31%|█▌   | 1.43G/4.54G [00:06<00:13, 239MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  32%|█▌   | 1.46G/4.54G [00:06<00:12, 239MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  33%|█▋   | 1.49G/4.54G [00:06<00:12, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  33%|█▋   | 1.52G/4.54G [00:06<00:12, 248MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  34%|█▋   | 1.55G/4.54G [00:06<00:12, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  35%|█▋   | 1.58G/4.54G [00:06<00:11, 247MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  36%|█▊   | 1.61G/4.54G [00:07<00:11, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  36%|█▊   | 1.65G/4.54G [00:07<00:11, 248MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  37%|█▊   | 1.68G/4.54G [00:07<00:11, 257MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  38%|█▉   | 1.71G/4.54G [00:07<00:11, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  38%|█▉   | 1.74G/4.54G [00:07<00:11, 244MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  39%|█▉   | 1.77G/4.54G [00:08<00:21, 128MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  39%|█▌  | 1.79G/4.54G [00:08<00:29, 94.7MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  40%|█▌  | 1.81G/4.54G [00:09<00:37, 73.6MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  40%|█▌  | 1.84G/4.54G [00:09<00:38, 69.9MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  41%|█▋  | 1.85G/4.54G [00:09<00:43, 62.5MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  41%|█▋  | 1.86G/4.54G [00:09<00:43, 61.3MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  41%|█▋  | 1.87G/4.54G [00:10<00:48, 55.5MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  41%|█▋  | 1.88G/4.54G [00:10<00:47, 56.0MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  42%|█▋  | 1.89G/4.54G [00:10<00:47, 55.3MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  42%|█▋  | 1.90G/4.54G [00:10<00:51, 51.1MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  42%|█▋  | 1.91G/4.54G [00:10<00:53, 49.2MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  42%|█▋  | 1.92G/4.54G [00:11<00:53, 48.8MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  42%|█▋  | 1.93G/4.54G [00:11<00:48, 53.3MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  43%|█▋  | 1.96G/4.54G [00:11<00:27, 94.5MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  44%|██▏  | 1.98G/4.54G [00:11<00:22, 115MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  44%|██▏  | 2.00G/4.54G [00:11<00:19, 129MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  45%|██▏  | 2.03G/4.54G [00:11<00:15, 163MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  45%|██▎  | 2.07G/4.54G [00:11<00:13, 190MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  46%|██▎  | 2.10G/4.54G [00:12<00:12, 203MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  47%|██▎  | 2.13G/4.54G [00:12<00:10, 222MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  48%|██▍  | 2.16G/4.54G [00:12<00:10, 219MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  48%|██▍  | 2.19G/4.54G [00:12<00:10, 226MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  49%|██▍  | 2.22G/4.54G [00:12<00:09, 232MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  50%|██▍  | 2.25G/4.54G [00:12<00:10, 226MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  50%|██▌  | 2.29G/4.54G [00:12<00:09, 228MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  51%|██▌  | 2.32G/4.54G [00:12<00:09, 240MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  52%|██▌  | 2.35G/4.54G [00:13<00:09, 241MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  52%|██▌  | 2.38G/4.54G [00:13<00:10, 210MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  53%|██▋  | 2.41G/4.54G [00:13<00:09, 227MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  54%|██▋  | 2.44G/4.54G [00:13<00:09, 233MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  55%|██▋  | 2.47G/4.54G [00:13<00:08, 236MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  55%|██▊  | 2.51G/4.54G [00:13<00:08, 245MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  56%|██▊  | 2.54G/4.54G [00:13<00:08, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  57%|██▊  | 2.57G/4.54G [00:14<00:08, 236MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  57%|██▊  | 2.60G/4.54G [00:14<00:07, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  58%|██▉  | 2.63G/4.54G [00:14<00:07, 247MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  59%|██▉  | 2.66G/4.54G [00:14<00:07, 240MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  59%|██▉  | 2.69G/4.54G [00:14<00:07, 252MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  60%|███  | 2.73G/4.54G [00:14<00:07, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  61%|███  | 2.76G/4.54G [00:14<00:07, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  61%|███  | 2.79G/4.54G [00:14<00:06, 264MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  62%|███  | 2.82G/4.54G [00:15<00:06, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  63%|███▏ | 2.85G/4.54G [00:15<00:06, 247MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  64%|███▏ | 2.88G/4.54G [00:15<00:06, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  64%|███▏ | 2.92G/4.54G [00:15<00:06, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  65%|███▏ | 2.95G/4.54G [00:15<00:06, 243MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  66%|███▎ | 2.98G/4.54G [00:15<00:06, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  66%|███▎ | 3.01G/4.54G [00:15<00:06, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  67%|███▎ | 3.04G/4.54G [00:15<00:06, 247MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  68%|███▍ | 3.07G/4.54G [00:16<00:05, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  68%|███▍ | 3.10G/4.54G [00:16<00:05, 248MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  69%|███▍ | 3.14G/4.54G [00:16<00:05, 238MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  70%|███▍ | 3.17G/4.54G [00:16<00:05, 238MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  70%|███▌ | 3.20G/4.54G [00:16<00:05, 240MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  71%|███▌ | 3.23G/4.54G [00:16<00:05, 243MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  72%|███▌ | 3.26G/4.54G [00:16<00:05, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  73%|███▋ | 3.29G/4.54G [00:16<00:05, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  73%|███▋ | 3.32G/4.54G [00:17<00:04, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  74%|███▋ | 3.36G/4.54G [00:17<00:04, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  75%|███▋ | 3.39G/4.54G [00:17<00:04, 245MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  75%|███▊ | 3.42G/4.54G [00:17<00:04, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  76%|███▊ | 3.45G/4.54G [00:17<00:04, 235MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  77%|███▊ | 3.48G/4.54G [00:17<00:04, 235MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  77%|███▊ | 3.51G/4.54G [00:17<00:04, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  78%|███▉ | 3.54G/4.54G [00:17<00:04, 244MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  79%|███▉ | 3.58G/4.54G [00:18<00:04, 234MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  79%|███▉ | 3.61G/4.54G [00:18<00:04, 218MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  80%|████ | 3.64G/4.54G [00:18<00:03, 233MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  81%|████ | 3.67G/4.54G [00:18<00:03, 236MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  82%|████ | 3.70G/4.54G [00:18<00:03, 244MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  82%|████ | 3.73G/4.54G [00:18<00:03, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  83%|████▏| 3.76G/4.54G [00:18<00:03, 248MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  84%|████▏| 3.80G/4.54G [00:19<00:02, 252MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  84%|████▏| 3.83G/4.54G [00:19<00:02, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  85%|████▏| 3.86G/4.54G [00:19<00:02, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  86%|████▎| 3.89G/4.54G [00:19<00:02, 241MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  86%|████▎| 3.92G/4.54G [00:19<00:02, 243MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  87%|████▎| 3.95G/4.54G [00:19<00:02, 238MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  88%|████▍| 3.98G/4.54G [00:19<00:02, 244MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  88%|████▍| 4.02G/4.54G [00:19<00:02, 247MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  89%|████▍| 4.05G/4.54G [00:20<00:02, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  90%|████▍| 4.08G/4.54G [00:20<00:01, 240MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  91%|████▌| 4.11G/4.54G [00:20<00:01, 240MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  91%|████▌| 4.14G/4.54G [00:20<00:01, 240MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  92%|████▌| 4.17G/4.54G [00:20<00:01, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  93%|████▋| 4.20G/4.54G [00:20<00:01, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  93%|████▋| 4.24G/4.54G [00:21<00:01, 160MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  94%|████▋| 4.27G/4.54G [00:21<00:01, 184MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  95%|████▋| 4.30G/4.54G [00:21<00:01, 199MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  95%|████▊| 4.33G/4.54G [00:21<00:01, 174MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  96%|████▊| 4.35G/4.54G [00:21<00:01, 113MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  96%|████▊| 4.37G/4.54G [00:22<00:01, 120MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  97%|████▊| 4.39G/4.54G [00:22<00:01, 124MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  97%|███▉| 4.41G/4.54G [00:22<00:01, 86.5MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  98%|███▉| 4.44G/4.54G [00:23<00:01, 71.4MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  98%|███▉| 4.45G/4.54G [00:23<00:01, 65.5MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  98%|███▉| 4.46G/4.54G [00:23<00:01, 62.3MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  98%|███▉| 4.47G/4.54G [00:23<00:01, 53.1MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  99%|███▉| 4.49G/4.54G [00:24<00:00, 62.4MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  99%|███▉| 4.50G/4.54G [00:24<00:00, 60.4MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  99%|███▉| 4.51G/4.54G [00:24<00:00, 59.7MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors: 100%|███▉| 4.52G/4.54G [00:24<00:00, 58.3MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors: 100%|███▉| 4.53G/4.54G [00:24<00:00, 62.1MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors: 100%|█████| 4.54G/4.54G [00:24<00:00, 182MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 3/3 [01:13<00:00, 24.56s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:03<00:00,  1.09s/it]\n",
      "generation_config.json: 100%|███████████████████| 111/111 [00:00<00:00, 476kB/s]\n",
      "[2024-05-02 13:57:30,627] [INFO] [axolotl.load_model:720] [PID:1827] [RANK:0] GPU memory usage after model load: 4.343GB (+0.146GB cache, +1.304GB misc)\u001b[39m\n",
      "[2024-05-02 13:57:30,633] [INFO] [axolotl.load_model:771] [PID:1827] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
      "[2024-05-02 13:57:30,635] [INFO] [axolotl.load_model:780] [PID:1827] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2024-05-02 13:57:30,638] [INFO] [axolotl.load_lora:924] [PID:1827] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'q_proj', 'k_proj', 'o_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n",
      "[2024-05-02 13:57:31,363] [INFO] [axolotl.load_model:825] [PID:1827] [RANK:0] GPU memory usage after adapters: 4.499GB (+1.122GB cache, +1.304GB misc)\u001b[39m\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[2024-05-02 13:57:31,374] [WARNING] [accelerate.utils.other.log:61] [PID:1827] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2024-05-02 13:57:31,552] [INFO] [axolotl.train.log:61] [PID:1827] [RANK:0] Pre-saving adapter config to ./mistral-out\u001b[39m\n",
      "[2024-05-02 13:57:31,597] [INFO] [axolotl.train.log:61] [PID:1827] [RANK:0] Starting trainer...\u001b[39m\n",
      "[2024-05-02 13:57:31,812] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "[2024-05-02 13:57:31,812] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "  0%|                                                    | 0/18 [00:00<?, ?it/s][2024-05-02 13:57:31,876] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "{'loss': 0.2077, 'grad_norm': 0.30078125, 'learning_rate': 2e-05, 'epoch': 1.0} \n",
      "  6%|██▍                                         | 1/18 [00:21<06:03, 21.40s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 13:57:59,363] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.38316038250923157, 'eval_runtime': 6.2004, 'eval_samples_per_second': 0.645, 'eval_steps_per_second': 0.323, 'epoch': 1.0}\n",
      "  6%|██▍                                         | 1/18 [00:27<06:03, 21.40s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.54s/it]\u001b[A\n",
      "                                                                                \u001b[A[2024-05-02 13:58:23,017] [INFO] [axolotl.callbacks.on_step_end:125] [PID:1827] [RANK:0] GPU memory usage while training: 4.672GB (+14.037GB cache, +1.677GB misc)\u001b[39m\n",
      "{'loss': 0.1985, 'grad_norm': 0.263671875, 'learning_rate': 4e-05, 'epoch': 2.0}\n",
      " 11%|████▉                                       | 2/18 [00:51<07:00, 26.31s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 13:58:29,187] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.378216028213501, 'eval_runtime': 6.2843, 'eval_samples_per_second': 0.637, 'eval_steps_per_second': 0.318, 'epoch': 2.0}\n",
      " 11%|████▉                                       | 2/18 [00:57<07:00, 26.31s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.57s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 13:58:31,909] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "{'loss': 0.1979, 'grad_norm': 0.25390625, 'learning_rate': 6e-05, 'epoch': 2.0} \n",
      " 17%|███████▎                                    | 3/18 [01:21<06:59, 27.99s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 13:58:59,223] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.35525888204574585, 'eval_runtime': 6.3321, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.316, 'epoch': 2.0}\n",
      " 17%|███████▎                                    | 3/18 [01:27<06:59, 27.99s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.1901, 'grad_norm': 0.1962890625, 'learning_rate': 8e-05, 'epoch': 3.0}\n",
      " 22%|█████████▊                                  | 4/18 [01:50<06:40, 28.63s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 13:59:28,861] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.3147001266479492, 'eval_runtime': 6.3474, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.315, 'epoch': 3.0}\n",
      " 22%|█████████▊                                  | 4/18 [01:57<06:40, 28.63s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 13:59:31,068] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "{'loss': 0.1624, 'grad_norm': 0.14453125, 'learning_rate': 0.0001, 'epoch': 3.0}\n",
      " 28%|████████████▏                               | 5/18 [02:20<06:16, 29.00s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 13:59:58,502] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.27555593848228455, 'eval_runtime': 6.3473, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.315, 'epoch': 3.0}\n",
      " 28%|████████████▏                               | 5/18 [02:26<06:16, 29.00s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.1489, 'grad_norm': 0.10986328125, 'learning_rate': 0.00012, 'epoch': 4.0}\n",
      " 33%|██████████████▋                             | 6/18 [02:50<05:50, 29.22s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:00:28,187] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.25245264172554016, 'eval_runtime': 6.3803, 'eval_samples_per_second': 0.627, 'eval_steps_per_second': 0.313, 'epoch': 4.0}\n",
      " 33%|██████████████▋                             | 6/18 [02:56<05:50, 29.22s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 14:00:30,426] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "{'loss': 0.1342, 'grad_norm': 0.11767578125, 'learning_rate': 0.00014, 'epoch': 4.0}\n",
      " 39%|█████████████████                           | 7/18 [03:19<05:23, 29.39s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:00:57,906] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.2382732480764389, 'eval_runtime': 6.3626, 'eval_samples_per_second': 0.629, 'eval_steps_per_second': 0.314, 'epoch': 4.0}\n",
      " 39%|█████████████████                           | 7/18 [03:26<05:23, 29.39s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.59s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.1137, 'grad_norm': 0.1298828125, 'learning_rate': 0.00016, 'epoch': 5.0}\n",
      " 44%|███████████████████▌                        | 8/18 [03:49<04:54, 29.46s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:01:27,531] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.22283808887004852, 'eval_runtime': 6.3595, 'eval_samples_per_second': 0.629, 'eval_steps_per_second': 0.314, 'epoch': 5.0}\n",
      " 44%|███████████████████▌                        | 8/18 [03:55<04:54, 29.46s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 14:01:29,762] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "{'loss': 0.1026, 'grad_norm': 0.126953125, 'learning_rate': 0.00018, 'epoch': 5.0}\n",
      " 50%|██████████████████████                      | 9/18 [04:19<04:25, 29.55s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:01:57,276] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.20396777987480164, 'eval_runtime': 6.3596, 'eval_samples_per_second': 0.629, 'eval_steps_per_second': 0.314, 'epoch': 5.0}\n",
      " 50%|██████████████████████                      | 9/18 [04:25<04:25, 29.55s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.1001, 'grad_norm': 0.11572265625, 'learning_rate': 0.0002, 'epoch': 6.0}\n",
      " 56%|███████████████████████▉                   | 10/18 [04:48<03:56, 29.62s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:02:27,044] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1904890239238739, 'eval_runtime': 6.3614, 'eval_samples_per_second': 0.629, 'eval_steps_per_second': 0.314, 'epoch': 6.0}\n",
      " 56%|███████████████████████▉                   | 10/18 [04:55<03:56, 29.62s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 14:02:29,527] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "{'loss': 0.0828, 'grad_norm': 0.09375, 'learning_rate': 0.0001923879532511287, 'epoch': 6.0}\n",
      " 61%|██████████████████████████▎                | 11/18 [05:18<03:28, 29.74s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:02:57,070] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.18162980675697327, 'eval_runtime': 6.3658, 'eval_samples_per_second': 0.628, 'eval_steps_per_second': 0.314, 'epoch': 6.0}\n",
      " 61%|██████████████████████████▎                | 11/18 [05:25<03:28, 29.74s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0746, 'grad_norm': 0.0908203125, 'learning_rate': 0.00017071067811865476, 'epoch': 7.0}\n",
      " 67%|████████████████████████████▋              | 12/18 [05:48<02:58, 29.74s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:03:26,807] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.17505759000778198, 'eval_runtime': 6.363, 'eval_samples_per_second': 0.629, 'eval_steps_per_second': 0.314, 'epoch': 7.0}\n",
      " 67%|████████████████████████████▋              | 12/18 [05:55<02:58, 29.74s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.59s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 14:03:29,415] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "{'loss': 0.0687, 'grad_norm': 0.080078125, 'learning_rate': 0.000138268343236509, 'epoch': 7.0}\n",
      " 72%|███████████████████████████████            | 13/18 [06:18<02:29, 29.85s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:03:56,900] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.17073509097099304, 'eval_runtime': 6.3691, 'eval_samples_per_second': 0.628, 'eval_steps_per_second': 0.314, 'epoch': 7.0}\n",
      " 72%|███████████████████████████████            | 13/18 [06:25<02:29, 29.85s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.59s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0544, 'grad_norm': 0.0712890625, 'learning_rate': 0.0001, 'epoch': 8.0}\n",
      " 78%|█████████████████████████████████▍         | 14/18 [06:48<01:59, 29.93s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:04:27,025] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.1654427945613861, 'eval_runtime': 6.3783, 'eval_samples_per_second': 0.627, 'eval_steps_per_second': 0.314, 'epoch': 8.0}\n",
      " 78%|█████████████████████████████████▍         | 14/18 [06:55<01:59, 29.93s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.59s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 14:04:29,747] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "{'loss': 0.0526, 'grad_norm': 0.059326171875, 'learning_rate': 6.173165676349103e-05, 'epoch': 8.0}\n",
      " 83%|███████████████████████████████████▊       | 15/18 [07:19<01:30, 30.02s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:04:57,253] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.16197210550308228, 'eval_runtime': 6.3664, 'eval_samples_per_second': 0.628, 'eval_steps_per_second': 0.314, 'epoch': 8.0}\n",
      " 83%|███████████████████████████████████▊       | 15/18 [07:25<01:30, 30.02s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.59s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0469, 'grad_norm': 0.0673828125, 'learning_rate': 2.9289321881345254e-05, 'epoch': 9.0}\n",
      " 89%|██████████████████████████████████████▏    | 16/18 [07:48<00:59, 29.96s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:05:27,066] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.15908634662628174, 'eval_runtime': 6.3757, 'eval_samples_per_second': 0.627, 'eval_steps_per_second': 0.314, 'epoch': 9.0}\n",
      " 89%|██████████████████████████████████████▏    | 16/18 [07:55<00:59, 29.96s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.59s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 14:05:29,237] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1827] [RANK:0] packing_efficiency_estimate: 0.9 total_num_tokens per device: 58383\u001b[39m\n",
      "{'loss': 0.048, 'grad_norm': 0.05322265625, 'learning_rate': 7.612046748871327e-06, 'epoch': 9.0}\n",
      " 94%|████████████████████████████████████████▌  | 17/18 [08:18<00:29, 29.89s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:05:56,794] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.15754958987236023, 'eval_runtime': 6.3677, 'eval_samples_per_second': 0.628, 'eval_steps_per_second': 0.314, 'epoch': 9.0}\n",
      " 94%|████████████████████████████████████████▌  | 17/18 [08:25<00:29, 29.89s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.59s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0392, 'grad_norm': 0.053466796875, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "100%|███████████████████████████████████████████| 18/18 [08:48<00:00, 29.87s/it]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A[2024-05-02 14:06:26,614] [INFO] [accelerate.accelerator.log:61] [PID:1827] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.15725085139274597, 'eval_runtime': 6.3662, 'eval_samples_per_second': 0.628, 'eval_steps_per_second': 0.314, 'epoch': 10.0}\n",
      "100%|███████████████████████████████████████████| 18/18 [08:54<00:00, 29.87s/it]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'train_runtime': 537.1212, 'train_samples_per_second': 1.005, 'train_steps_per_second': 0.034, 'train_loss': 0.11240819158653419, 'epoch': 10.0}\n",
      "100%|███████████████████████████████████████████| 18/18 [08:57<00:00, 29.84s/it]\n",
      "[2024-05-02 14:06:40,499] [INFO] [axolotl.train.log:61] [PID:1827] [RANK:0] Training Completed!!! Saving pre-trained model to ./mistral-out\u001b[39m\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "adapter_model.bin: 100%|█████████████████████| 168M/168M [00:07<00:00, 23.6MB/s]\n",
      "(PeftModelForCausalLM(   (base_model): LoraModel(     (model): MistralForCausalLM(       (model): MistralModel(         (embed_tokens): Embedding(32000, 4096)         (layers): ModuleList(           (0-31): 32 x MistralDecoderLayer(             (self_attn): MistralAttention(               (q_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (k_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=1024, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (v_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=1024, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (o_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (rotary_emb): MistralRotaryEmbedding()             )             (mlp): MistralMLP(               (gate_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=14336, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (up_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=14336, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (down_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=14336, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (act_fn): SiLU()             )             (input_layernorm): MistralRMSNorm()             (post_attention_layernorm): MistralRMSNorm()           )         )         (norm): MistralRMSNorm()       )       (lm_head): Linear(in_features=4096, out_features=32000, bias=False)     )   ) ), LlamaTokenizer(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), })\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !accelerate launch -m axolotl.cli.train config.yml\n",
    "!accelerate launch -m axolotl.cli.train qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65fef472-772f-4f21-9c19-663d0f9f76b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-02 14:08:03,772] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "                                 dP            dP   dP \n",
      "                                 88            88   88 \n",
      "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
      "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
      "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
      "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
      "                                                       \n",
      "                                                       \n",
      "\n",
      "****************************************\n",
      "**** Axolotl Dependency Versions *****\n",
      "  accelerate: 0.28.0         \n",
      "        peft: 0.10.0         \n",
      "transformers: 4.40.0.dev0    \n",
      "         trl: 0.8.5          \n",
      "       torch: 2.1.2          \n",
      "bitsandbytes: 0.43.0         \n",
      "****************************************\n",
      "\u001b[33m[2024-05-02 14:08:04,927] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_wo_flash:669] [PID:2405] [RANK:0] sample_packing without flash_attention or sdp_attention does not handle cross-attention.\u001b[39m\n",
      "[2024-05-02 14:08:04,928] [DEBUG] [axolotl.normalize_config:79] [PID:2405] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 14:08:05,162] [INFO] [axolotl.normalize_config:182] [PID:2405] [RANK:0] GPU memory usage baseline: 0.000GB (+0.456GB misc)\u001b[39m\n",
      "[2024-05-02 14:08:05,164] [INFO] [axolotl.common.cli.load_model_and_tokenizer:50] [PID:2405] [RANK:0] loading tokenizer... mistralai/Mistral-7B-Instruct-v0.2\u001b[39m\n",
      "[2024-05-02 14:08:05,492] [DEBUG] [axolotl.load_tokenizer:279] [PID:2405] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2024-05-02 14:08:05,492] [DEBUG] [axolotl.load_tokenizer:280] [PID:2405] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2024-05-02 14:08:05,492] [DEBUG] [axolotl.load_tokenizer:281] [PID:2405] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2024-05-02 14:08:05,492] [DEBUG] [axolotl.load_tokenizer:282] [PID:2405] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2024-05-02 14:08:05,492] [INFO] [axolotl.load_tokenizer:293] [PID:2405] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-05-02 14:08:05,492] [INFO] [axolotl.common.cli.load_model_and_tokenizer:52] [PID:2405] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
      "[2024-05-02 14:08:06,780] [INFO] [accelerate.utils.modeling.get_balanced_memory:965] [PID:2405] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.29it/s]\n",
      "[2024-05-02 14:08:09,441] [INFO] [axolotl.load_model:720] [PID:2405] [RANK:0] GPU memory usage after model load: 13.989GB (+0.251GB cache, +1.304GB misc)\u001b[39m\n",
      "[2024-05-02 14:08:09,447] [INFO] [axolotl.load_model:780] [PID:2405] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2024-05-02 14:08:09,449] [INFO] [axolotl.load_lora:924] [PID:2405] [RANK:0] found linear modules: ['q_proj', 'gate_proj', 'o_proj', 'k_proj', 'v_proj', 'up_proj', 'down_proj']\u001b[39m\n",
      "[2024-05-02 14:08:09,449] [DEBUG] [axolotl.load_lora:966] [PID:2405] [RANK:0] Loading pretrained PEFT - LoRA\u001b[39m\n",
      "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n",
      "[2024-05-02 14:08:12,316] [INFO] [axolotl.load_model:825] [PID:2405] [RANK:0] GPU memory usage after adapters: 14.145GB (+1.384GB cache, +1.304GB misc)\u001b[39m\n",
      "[2024-05-02 14:08:12,316] [INFO] [axolotl.scripts.do_merge_lora:144] [PID:2405] [RANK:0] running merge of LoRA with base model\u001b[39m\n",
      "Unloading and merging model: 100%|███████████| 678/678 [00:00<00:00, 758.42it/s]\n",
      "[2024-05-02 14:08:13,215] [INFO] [axolotl.scripts.do_merge_lora:153] [PID:2405] [RANK:0] saving merged model to: mistral-out/merged\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m axolotl.cli.merge_lora qlora.yml --lora_model_dir=\"./mistral-out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607ec49-9490-4869-b3ca-1d591457fffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-02 14:13:07,971] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "                                 dP            dP   dP \n",
      "                                 88            88   88 \n",
      "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
      "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
      "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
      "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
      "                                                       \n",
      "                                                       \n",
      "\n",
      "****************************************\n",
      "**** Axolotl Dependency Versions *****\n",
      "  accelerate: 0.28.0         \n",
      "        peft: 0.10.0         \n",
      "transformers: 4.40.0.dev0    \n",
      "         trl: 0.8.5          \n",
      "       torch: 2.1.2          \n",
      "bitsandbytes: 0.43.0         \n",
      "****************************************\n",
      "[2024-05-02 14:13:09,495] [DEBUG] [axolotl.normalize_config:79] [PID:2599] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 14:13:09,701] [INFO] [axolotl.normalize_config:182] [PID:2599] [RANK:0] GPU memory usage baseline: 0.000GB (+0.456GB misc)\u001b[39m\n",
      "[2024-05-02 14:13:10,518] [INFO] [matplotlib.font_manager._load_fontmanager:1578] [PID:2599] generated new fontManager\n",
      "[2024-05-02 14:13:11,115] [INFO] [axolotl.common.cli.load_model_and_tokenizer:50] [PID:2599] [RANK:0] loading tokenizer... mistralai/Mistral-7B-Instruct-v0.2\u001b[39m\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-02 14:13:11,435] [DEBUG] [axolotl.load_tokenizer:279] [PID:2599] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2024-05-02 14:13:11,435] [DEBUG] [axolotl.load_tokenizer:280] [PID:2599] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2024-05-02 14:13:11,435] [DEBUG] [axolotl.load_tokenizer:281] [PID:2599] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2024-05-02 14:13:11,435] [DEBUG] [axolotl.load_tokenizer:282] [PID:2599] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2024-05-02 14:13:11,436] [INFO] [axolotl.load_tokenizer:293] [PID:2599] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-05-02 14:13:11,436] [INFO] [axolotl.common.cli.load_model_and_tokenizer:52] [PID:2599] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
      "[2024-05-02 14:13:13,513] [INFO] [accelerate.utils.modeling.get_balanced_memory:965] [PID:2599] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "[2024-05-02 14:13:16,483] [INFO] [axolotl.load_model:720] [PID:2599] [RANK:0] GPU memory usage after model load: 4.343GB (+0.146GB cache, +1.304GB misc)\u001b[39m\n",
      "[2024-05-02 14:13:16,488] [INFO] [axolotl.load_model:771] [PID:2599] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
      "[2024-05-02 14:13:16,491] [INFO] [axolotl.load_model:780] [PID:2599] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2024-05-02 14:13:16,493] [INFO] [axolotl.load_lora:924] [PID:2599] [RANK:0] found linear modules: ['gate_proj', 'q_proj', 'o_proj', 'up_proj', 'v_proj', 'down_proj', 'k_proj']\u001b[39m\n",
      "[2024-05-02 14:13:16,493] [DEBUG] [axolotl.load_lora:966] [PID:2599] [RANK:0] Loading pretrained PEFT - LoRA\u001b[39m\n",
      "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n",
      "[2024-05-02 14:13:18,877] [INFO] [axolotl.load_model:825] [PID:2599] [RANK:0] GPU memory usage after adapters: 4.499GB (+1.278GB cache, +1.304GB misc)\u001b[39m\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://793d1b580b97eb45ba.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    }
   ],
   "source": [
    "!python -m axolotl.cli.inference qlora.yml --lora_model_dir=\"./mistral-out\" --gradio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
